\documentclass[12pt,letterpaper]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{enumitem}

\input{macros.tex}

% info for header block in upper right hand corner
\name{Justin Jiang}
\class{Math189R SU20}
\assignment{Homework 1}
\duedate{Wednesday, Feb 4, 2020}

\renewcommand{\labelenumi}{{(\alph{enumi})}}


\begin{document}
Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
The starter code for problem 2 part c and d can be found under the Resource tab on course website.\\

\textit{Note:} You need to create a Github account for submission of the coding part of the homework. Please create a repository on Github to hold all your code and include your Github account username as part of the answer to problem 2.

\begin{problem}[1]
(\textbf{Linear Transformation}) Let $\mathbf{y} = A\mathbf{x} + \mathbf{b}$ be a random vector.
show that expectation is linear:
\[
    \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb.
\]
Also show that
\[
    \cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.
\]
\end{problem}
\begin{solution}
    a) To show that A\EE[\xx] + \bb  is linear, we first need to integrate the given statement \EE[\yy] = \EE[A\xx + \bb]  such that \EE[\yy] =  $\int_{S} \mathbf{y} P(x) dx$. Since $\mathbf{y} = A\mathbf{x} + \mathbf{b}$, \EE[\yy] =  $\int_{S} (A\mathbf{x} + \mathbf{b}) P(x) dx$.\\ 
    
    Now, factor in the P(x), then split the intergral across the addition, such that: \\
                                          $\int_{S} (A\mathbf{x}) P(x) dx+ \int_{S} \mathbf{b} P(x) dx$\\
    Next, factor out the constants A and  $\mathbf{b}$, such that:\\ A$\int_{S} (\mathbf{x}) P(x) dx+ \mathbf{b} \int_{S}  P(x) dx$ \\
    This simplifies to: A\EE[\xx] + \bb, since the integral $\int_{S} (\mathbf{x}) P(x) dx$ = \EE[\xx]
   \\
   \\
   b) Since $\mathbf{y} = A\mathbf{x} + \mathbf{b}$,  \cov[\yy] = \cov[A\xx + \bb]. \\
   Per the definition of covariance, $ \cov[\xx] = \EE[(x - \EE[\xx])(x - \EE[\xx])^\T  ] $ \\
   Thus, $\cov[\yy] = \cov[A\xx + \bb] =  \EE[(Ax + b - \EE[Ax +b])( Ax + b - \EE[Ax + b])^\T $ \\
   Per a), we can factor out the internal \EE[Ax+b], such that: $\EE[(Ax + b - A\EE[x] - b)( Ax + b - A\EE[x] - b])^\T $\\
   Combine terms to get: $\EE[(Ax - A\EE[x])(Ax - A\EE[x]^\T] $  \\
   Factor out A: $A \EE[(x - \EE[x])(x - \EE[x]^\T] A^\T $  \\
   Per the definition of the covariance, $\EE[(x - \EE[x])(x - \EE[x]^\T] = \cov[\xx]$ \\
   Therefore, the above statement is equal to: $ A \cov[\xx] A^\T $ \\
   Which is equal to $A\Sigmab A^\T $, per definition of the covariance. QED.
 
   	
     					

\end{solution}

\begin{problem}[2]
Given the dataset $\Dc = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
\begin{enumerate}
   \item Find the least squares estimate $y = \thetab^\T\xx$ by hand using
        Cramer's Rule.
    \item Use the normal equations to find the same solution and verify it
        is the same as part (a).
    \item Plot the data and the optimal linear fit you found.
    \item Find randomly generate 100 points near the line with white Gaussian
        noise and then compute the least squares estimate (using a computer).
        Verify that this new line is close to the original and plot the new
        dataset, the old line, and the new line.
\end{enumerate}

\end{problem}
\begin{solution}
 a) X = $\begin{pmatrix}				
1 & 0 \\								
1 & 2 \\
1 & 3 \\
1 & 4 
\end{pmatrix}$  
$\yy = \begin{pmatrix} 
1 \\
3\\
6\\
8
\end{pmatrix}$
Therefore, $X^\T X = \begin{pmatrix}
4 & 9 \\
9 & 29 \\
\end{pmatrix}$
\end{solution} and $X^\T \yy = \begin{pmatrix}
18\\
56
\end{pmatrix}$

Now, to find the 2x1 $\theta $ matrix, we need to use Cramer's rule, such that: \\

$\theta_0 = \det[(18, 9)(56, 29)] / \det[(4, 9)(9, 29)]  = 18/35  $ \\
$\theta_1 = \det[(4, 18)(9, 56)] / \det[(4, 9)(9, 29)]  = 62/35  $ \\
\\
\\
b) The normal equation, per the lecture notes, is $\theta^* = inv((X^\T X)) X ^\T \yy        $\\
Using the matricies from part a, and plugging them into Matlab (sorry, linalg triggers me), we get that $\theta^* = \begin{pmatrix} 18/35 \\ 62/35 \end{pmatrix}$ , as per part a.\\ \\ 

For part c and d, I included the graphs as part of my submission to github.

\newpage



\end{document}

