\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{Justin Jiang}
\class{Math189R SU2020}
\assignment{Homework 7}
\duedate{}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files for problem 2 can be found under the Resource tab on course website. The plot for problem 2 generated by the sample solution has been included in the starter files for reference. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.

\begin{problem}[1]
\textbf{(Murphy 11.3 - EM for Mixtures of Bernoullis)} Show that the M step for ML estimation
of a mixture of Bernoullis is given by
\[
    \mu_{kj} = \frac{\sum_i r_{ik}x_{ij}}{\sum_i r_{ik}}.
\]
Show that the M step for MAP estimation of a mixture of Bernoullis with a $\beta(a,b)$ prior
is given by
\[
    \mu_{kj} = \frac{\left(\sum_i r_{ik}x_{ij}\right) + a - 1}{\left(\sum_i r_{ik}\right) + a + b - 2}.
\]
\end{problem}
\begin{solution}
Had to peek at the solutions for help on this problem:

The data log likelihood is:
\[ \sum_i \sum_k r_{ik} \sum_j \xx_{ij} log \mu_{kj} + (1 - \xx_{ij})log(1-mu_{kj}) \]
Now, take the derivative with respect to $\mu_{kj}$, we get: \[
\frac{\partial l}{\partial \mu_{kj}} = \sum_i r_{ik} (\frac{\xx_{ij}}{\mu_{kj}} - \frac{1-\xx_{ij}}{1-\xx_{kj}})
\] 
\[ = \frac{1}{\mu_{kj}(1-\mu_{kj})} \sum_i r_{ik} (\xx_{ij} - \mu_{kj} ) = 0 \]
Which gives: \[
\sum_i r_{ik} \xx_{ij} = \mu_{kj} \sum_i r_{ik} \]

\end{solution}
\newpage



\begin{problem}[2]
\textbf{(Lasso Feature Selection)} 
In this problem, we will use the online news popularity dataset we used in hw2pr3. In the starter code, we have already parsed the data for you. However, you might need internet connection to access the data and therefore successfully run the starter code.
\newline
\newline
First, ignoring undifferentiability at $x=0$, take $\frac{\partial |x|}{\partial x}
= \mathrm{sign} (x)$. Using this, show that $\nabla \|\xx\|_1 = \mathrm{sign}(\xx)$ where $\mathrm{sign}$ is applied
elementwise. Derive the gradient of the $\ell_1$ regularized linear regression objective
\begin{align*}
    \text{minimize: } & \|A\xx - \bb\|_2^2 + \lambda \|\xx\|_1
\end{align*}

Then, implement a gradient descent based solution of the above optimization problem for this data. Produce
the convergence plot (objective vs. iterations) for a non-trivial value of $\lambda$.
In the same figure (and different axes) produce a `regularization path' plot. Detailed
more in section 13.3.4 of Murphy, a regularization path is a plot of the optimal weight on
the $y$ axis at a given regularization strength $\lambda$ on the $x$ axis. Armed with this
plot, provide an ordered list of the top five features in predicting the log-shares of a news
article from this dataset (with justification).
\end{problem}
\begin{solution}
I've included the code and graphs as part of my submission to github. 
\end{solution}
\newpage

\end{document}
